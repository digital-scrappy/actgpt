{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f135cfb-1de5-4dee-ad9d-3995de240775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from paths import DATA_DIR, TOK_LOC\n",
    "from hyperparameters import (BATCH_SIZE, BLOCK_SIZE, DROPOUT, EVAL_INTERVAL,\n",
    "                             EVAL_ITERS, LEARNING_RATE, MAX_ITERS, N_EMBD,\n",
    "                             N_HEAD, N_LAYER, VOCAB_SIZE)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cdb96f-b370-477a-8ac0-9a06b3d8720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7ccd58-fbaf-406a-8e93-d0105e8951ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(str(TOK_LOC))\n",
    "with open(DATA_DIR / \"train.txt\", \"r\") as f:\n",
    "    train_enc = tokenizer.encode(f.read())\n",
    "with open(DATA_DIR / \"val.txt\", \"r\") as f:\n",
    "    val_enc = tokenizer.encode(f.read())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df2cfdbd-c1ce-4444-80e0-41d4edfd6239",
   "metadata": {
    "tags": []
   },
   "source": [
    "def build_ngrams(token_ids):\n",
    "    X, Y = [], []\n",
    "    #\"[SOS]\"  should have id 0 \n",
    "    context = [0 for i in range(BLOCK_SIZE)]\n",
    "    for token_id in token_ids:\n",
    "        X.append(context)\n",
    "        Y.append(token_id)\n",
    "\n",
    "        context = context[1:] + [token_id]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0888484b-8255-4615-a1ca-29723463899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(split):\n",
    "    data = train_enc.ids if split == 'train' else val_enc.ids\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    #torch.tensor uses ints and torch.Tensor uses float also torch.Tensor is annoying\n",
    "    x = torch.stack([torch.tensor(data[i:i+BLOCK_SIZE]) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+BLOCK_SIZE+1]) for i in ix])\n",
    "    X, Y = x.to(device), y.to(device)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a76be58-45e0-4732-a5db-1863b01d68f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, VOCAB_SIZE)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "        \n",
    "            B, T, C = logits.shape\n",
    "        \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18cd426d-77bc-4861-b2fc-0e0341011d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 512])\n",
      "tensor(6.7543, grad_fn=<NllLossBackward0>)\n",
      "##mm ##l ##a  ##Q ## ##7 sch man ##ì ##hn ##r und ##: ##pf û ##as ##au Sch ##ei ##ort C X ##R ##der ##aß ver ##q ##' ##ill ##> ##ck ##á ##-- wer ##re ##M ##es aber ##keit ##uß ##ô A ##al ##à dies é ##in ##ber ##ind ## ##o ##und ##as mir ##E ##el ##em , | ##v ##ot im bei Ich v ##mm ##ig ##z 0 ##zu ~ 1 hat h ##b ##bst ##us m ##hn ù ##us ##rei D ##- all ##| er and ##gt ##u , ##ir C à Q ##Y kann einen\n"
     ]
    }
   ],
   "source": [
    "xb, yb = build_batch(\"train\")\n",
    "m = BigramLanguageModel()\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(tokenizer.decode(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8f293c-5b44-48d8-b37e-5bd2303f7b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206e438c-89ce-4010-b08c-d2c6a8141642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.762286186218262\n",
      "6.834046840667725\n",
      "6.696063041687012\n",
      "6.807833194732666\n",
      "6.811186790466309\n",
      "6.784790992736816\n",
      "6.748086452484131\n",
      "6.733273506164551\n",
      "6.82156229019165\n",
      "6.741987705230713\n",
      "6.8552656173706055\n",
      "6.767934322357178\n",
      "6.720228672027588\n",
      "6.750064849853516\n",
      "6.675497531890869\n",
      "6.74094295501709\n",
      "6.753795623779297\n",
      "6.741440773010254\n",
      "6.742257118225098\n",
      "6.7121992111206055\n",
      "6.854103088378906\n",
      "6.766846656799316\n",
      "6.79730749130249\n",
      "6.7569732666015625\n",
      "6.8162946701049805\n",
      "6.874232292175293\n",
      "6.691197872161865\n",
      "6.768557548522949\n",
      "6.698671817779541\n",
      "6.85129451751709\n",
      "6.677887916564941\n",
      "6.760354518890381\n",
      "6.714031219482422\n",
      "6.755202293395996\n",
      "6.784643650054932\n",
      "6.720249176025391\n",
      "6.711251258850098\n",
      "6.615055084228516\n",
      "6.733750820159912\n",
      "6.762115478515625\n",
      "6.772369384765625\n",
      "6.661929130554199\n",
      "6.7191009521484375\n",
      "6.640076637268066\n",
      "6.604544162750244\n",
      "6.8138604164123535\n",
      "6.757739067077637\n",
      "6.734800815582275\n",
      "6.757530212402344\n",
      "6.780100345611572\n",
      "6.690114498138428\n",
      "6.741968631744385\n",
      "6.708401679992676\n",
      "6.706373691558838\n",
      "6.770060062408447\n",
      "6.695371150970459\n",
      "6.701485633850098\n",
      "6.7438530921936035\n",
      "6.712346076965332\n",
      "6.685173988342285\n",
      "6.767611026763916\n",
      "6.762235164642334\n",
      "6.691220760345459\n",
      "6.66709041595459\n",
      "6.691796779632568\n",
      "6.803868770599365\n",
      "6.7816314697265625\n",
      "6.6012468338012695\n",
      "6.790126800537109\n",
      "6.814840793609619\n",
      "6.765146732330322\n",
      "6.747838497161865\n",
      "6.7324113845825195\n",
      "6.683114051818848\n",
      "6.654850006103516\n",
      "6.642317771911621\n",
      "6.6609907150268555\n",
      "6.699045658111572\n",
      "6.743838310241699\n",
      "6.7486572265625\n",
      "6.756458282470703\n",
      "6.804074287414551\n",
      "6.640406608581543\n",
      "6.660733699798584\n",
      "6.8076171875\n",
      "6.66220235824585\n",
      "6.695784091949463\n",
      "6.710370063781738\n",
      "6.687127590179443\n",
      "6.611563682556152\n",
      "6.68194055557251\n",
      "6.6691484451293945\n",
      "6.666919708251953\n",
      "6.574646949768066\n",
      "6.746752738952637\n",
      "6.754067897796631\n",
      "6.604001045227051\n",
      "6.666984558105469\n",
      "6.691789150238037\n",
      "6.672183990478516\n"
     ]
    }
   ],
   "source": [
    "for steps in range(100):\n",
    "    \n",
    "    xb, yb = build_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad02ab3d-b2a1-4adb-803d-5b7a01cb3687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a072b-ecc4-4b82-98a9-60b53d92eb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2f108-9bb5-4721-86e5-7c384be5e308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
